# Contexto del Proyecto: Luz (SEC Scraper & ETL)

## Objetivo

Construir un sistema de datos históricos sobre cortes de luz en Chile (2017-Presente) usando datos públicos de la SEC.

## Arquitectura Actual

1.  **Scraper (`core/scraper.py`):**
    - Usa Playwright (interceptación de red).
    - Intercepta peticiones POST a `GetPorFecha`.
    - Capaz de "viajar en el tiempo" inyectando payloads JSON con fechas pasadas.
2.  **Transformer (`core/transformer.py`):**
    - Normaliza datos (Mayúsculas, Strip).
    - **Lógica Clave:** Calcula `DIAS_ANTIGUEDAD` restando la fecha del payload (`server_time`) menos la fecha del incidente (`FECHA_INT_STR`).
    - **Ignora:** El campo `ACTUALIZADO_HACE` de la SEC (es basura en datos históricos).
    - **Agregación:** Suma filas fragmentadas ("Efecto Puerto Montt") agrupando por Comuna+Empresa+Fecha.
    - **ID Único:** Hash MD5 (`Comuna + Empresa + Timestamp_Captura`) para snapshots históricos.

## Estado Actual

- Scraper: ✅ Listo y probado. Detectado capability de Backfill (2017+).
- Transformer: ✅ Lógica lista. **Pendiente:** Finalizar `tests/test_transformer.py`.
- Database: ⏳ Pendiente (Supabase/PostgreSQL).

## Tarea Inmediata

Implementar los 4 tests unitarios críticos en `tests/test_transformer.py` para validar:

1.  **Agregación:** Que múltiples filas de una comuna se sumen.
2.  **Limpieza:** Que strings con espacios se normalicen.
3.  **Matemática:** Que `DIAS_ANTIGUEDAD` se calcule bien incluso cruzando años/meses.
4.  **Consistencia:** Que el mismo input genere el mismo Hash ID.

## Reglas de Oro

- No usar `ACTUALIZADO_HACE` para lógica.
- El ID es un snapshot, no un evento único deduplicado (eso se hará en SQL Views).
- `server_time` es la fuente de verdad temporal.
